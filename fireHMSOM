#A Bayesian Hierarchical Multi Species Occupancy model written using the 'nimble' language and MCMC sampler.

#I first provide the model script with detailed annotations, and then below it I provide only the model script itself
#just in case it isn't clear what are annotations and what are actual model components. Annotations always start with a hashtag #, 
#but I realise I've written a lot of annotations, so the actual model code has become a bit disjointed in the first section! Sorry :)

#####The Annotated Model#####

fireMod <- nimbleCode({       #This line just defines the model as a 'nimble' package object
  
  #####Likelihood#####        #The section below defines the model likelihood, i.e. the effects of our predictor variables on our outcomes of interest 
                              #i.e., species occurence and detection probabilities
  
  for(i in 1:nSpObs){         #We estimate occurrence and detection parameters for every species in the data set individually
    
    for(j in 1:nSites){       #We estimate species occurrence and detection parameters at each site independently
      
      for(y in 1:nYears){     #We also estimate occurrence and detection probabilities at each site in each year independently, but...
                              #occurrence probabilities for each species in each site will be linked to each other across years.
                              #This works on the rational that if a species occurred at a site in year one, it is more likely to occur at that site in year two
                              #(assuming that the site hasn't been burnt, logged etc., but the model will account for temporal changes in these factors too)
                              
                              #N.B.: This model assumes that the effects of covariates (i.e., burning, logging, etc.) on each species are consistent across years
                              #This is a fair assumption and if we did allow these effects to vary between years it would likely result in a very complex model with very uncertain results
        
        if(y == 1){           #Here we define the occurence model for the first year of samping
                              #we symbolise 'year of sampling' as y, and if y == 1 then the code will perform the below model
          
          #Here (below) is our first occurence model, i.e., that for year 1
            #psi[i,j,y] = The probability of species i occuring at site j in year y. We calculate this on the logit scale as probabilities are constrained to values between 0 and 1,
                          #but this is problematic when trying to estimate the effect sizes of predictor variables. Logit transforming the probabilities puts them on a continuous scale
                          #i.e., they can range between -∞ and ∞, and we can then back transform to true probability values (0-1)
            #abar[i] = The standard model intercept parameter for species i, this is species-specific but is always the same regardless of year or site
            #a[i,j,y] = The spatial autocorrelation parameter. I have let this vary by year, as in my research ongoing deforestation may result in sites becoming further apart over time
                        #However, if your camera traps were always placed at the same spot every year, then it might not need to be calculated independently for each year
                        #In my opinion, this whole parameter/concept is somewhat questionable. 
                        #It works on the assumption that species are more likely to occur in sites that are closer to other sites, 
                        #but it doesn't take into account whether or not the species actually occured at the nearby sites.
                        #There are ways to take into account whether species occurr in neighbouring sites, but these methods are also questionable 
                        #as the whole point of this model is that we don't know for sure if a species actually does occur at any of the sites (unless we actually see them)!
                        #To be honest, I found the concept of this process cool so I wanted to see if I could code it, but it might not be worth using. 
                        #There are ways to determine whether or not including this parameter improves model performance, which I can help with.
            #bOCV[i,1:nOCV] = A 2D matrix (species by covariate) containing the slope (b or beta) parameters for each of our predictor variables. 
                              #These are calculated independently for each species i. 
                              #We can have as many predictors as we want, so to keep the model tidy I have included this as 1:nOCV,
                              #where nOCV represents the number of predictors [number(n) of occurence(O) covariates(CV)].
            #PatchOCV[j,y,1:nOCV] = A 3D matrix (Sites by year by covariate) containing the values of the predictor variables at site j in year y     
            #'inprod' is simply a tidy way of multiplying multiple values (i.e, each beta estimate multiplied by its corresponding site-specific parameter value) 
          
          logit(psi[i,j,y]) <- abar[i] + a[i,j,y] + inprod(bOCV[i,1:nOCV], PatchOCV[j,y,1:nOCV])   #We don't have any knowledge of species occurrence before year 1, as sampling hadn't started then
                                                                                                   #Therefore, we just estimate species occurence probabilities as the outcome of our predictor variables 
                                                                                                   #(e.g., burn status, average temperature, etc. - whatever variables you think might effect a species' occurence can be included here) 
                                                                                                   #You also don't necessarily have to include the same variables for each species, although we don't want to make the model too complex 
                                                                                                   #(e.g., the presence of fruiting trees may have an effect on frugivorous rodents, but there's little reason it would interest carnivorous jaguars)
          
          
        } else {      #If the data isn't from the first year of sampling then we perform this second occurence model instead
          
          #The only difference here is the 'theta' parameter
          #theta[i] = The increase in occurence probability (on the logit scale) if species i DID occur within site j the year before (y-1)
          
          logit(psi[i,j,y]) <- abar[i] + a[i,j,y] + inprod(bOCV[i,1:nOCV], PatchOCV[j,y,1:nOCV]) + theta[i] * z[i,j,(y-1)]
          
        }
        
        z[i,j,y] ~ dbern(psi[i,j,y])
        
        for(s in 1:nTrapWeeks[j,y]){
          
          logit(p[i,j,s,y]) <- lp[i,y] + inprod(bDCV[i,1:nDCV[y],y], SampleDCV[j,s,y,1:nDCV[y]])    
          y[i,j,s,y] ~ dbern(p[i,j,s,y] * z[i,j,y])
          
        }
      }        #That is the end of the model likelihood section, now we move onto priors
      
      #####Species-Level Priors##### 
      
      #Spatial autocorrelation - Gaussian Process Intercept
      alpha2[i] ~ dexp(1)
      rho2[i] ~ dexp(1)
      delta <- 0.01
      
      for(j in 1:nPatches){
        for(y in 1:nYears){
          z_score[i,j,y] ~ dnorm(0, sd = 1)
        }
      }
      
      for(y in 1:nYears){
        for(j in 1:(nPatches - 1)){
          
          sigma[j,j,i,y] <- alpha2[i] + delta
          
          for(l in (j+1):nPatches){
            
            sigma[j,l,i,y] <- alpha2[i] * exp(-rho2[i] * DMat2[j,l,y])
            sigma[l,j,i,y] <- sigma[j,l,i,y]
            
          }
        }
        
        sigma[nPatches,nPatches,i,y] <- alpha2[i] + delta
        sigma_cd[,,i,y] <- chol(sigma[,,i,y])
        a[i,,y] <- sigma_cd[,,i,y] %*% z_score[i,,y]
        
      }
      
      #Temporal autocorrelation
      theta[i] ~ dnorm(mu.theta, sd = sd.theta)
      
      #Occurence coefficients
      abar[i] ~ dnorm(mu.abar, sd = sd.abar)
      for(n in 1:nOCV){ bOCV[i,n] ~ dnorm(mu.OCV[n], sd = sd.OCV[n]) }
      
      #Detection coefficients
      #Integrated model - separate detection parameters for each sampling method 
      for(y in 1:nYears){
        
        mu.eta[i,y] <- mu.lp[y] + rho[y] * sd.lp[y]/sd.abar * (abar[i] - mu.abar)
        lp[i,y] ~ dnorm(mu.eta[i,y], sd = sd.eta[y])
        
        for(n in 1:nDCV[y]){ 
          
          bDCV[i,n,y] ~ dnorm(mu.DCV[n,y], sd = sd.DCV[n,y]) 
          
        }
      }
    }
    
    #####Community Hyperparameters#####
    
    abar.mean ~ dbeta(1, 1)
    mu.abar <- logit(abar.mean)
    sd.abar ~ dunif(0, 5)
    
    theta.mean ~ dnorm(0, 0.1)
    mu.theta ~ logit(theta.mean)
    sd.theta ~ dunif(0, 5)
    
    for(y in 1:nYears){
      
      p.mean[y] ~ dbeta(1, 1)
      mu.lp[y] <- logit(p.mean[y])
      sd.lp[y] ~ dunif(0, 5)
      
      rho[y] ~ dunif(-1, 1) 
      sd.eta[y] <- sd.lp[y]/rho[y]
      
      for(n in 1:nDCV[y]){
        
        mu.DCV[n,y] ~ dnorm(0, 0.1)
        sd.DCV[n,y] ~ dunif(0, 5)
        
      }
    }
    
    for(n in 1:nOCV){
      
      mu.OCV[n] ~ dnorm(0, 0.1)
      sd.OCV[n] ~ dunif(0, 5)
      
    }
    
    #####Derived Quantities#####
    
    for(j in 1:nPatches){
      
      for(y in 1:nYears){ PatchR[j,y] <- sum(z[,j,y]) }
      
      aEra <- sum(z[,j,1] * z[,j,2])
      bEra <- sum(z[,j,1] > z[,j,2])
      cEra <- sum(z[,j,2] > z[,j,1])
      
      EraSor[j] <- (bEra + cEra) / (2 *  + bEra + cEra)
      EraTurn[j] <- min(bEra, cEra) / (aEra + min(bEra, cEra))
      EraNest[j] <- EraSor[j] - EraTurn[j]
      
    }
    
    for(y in 1:nYears){
      for(j in 1:nPatches){
        
        PairSor[j,j,y] <- 0
        PairTurn[j,j,y] <- 0
        PairNest[j,j,y] <- 0
        
        for(l in (j+1):nPatches){
          
          aPair <- sum(z[,j,y] * z[,l,y])
          bPair <- sum(z[,j,y] > z[,l,y])
          cPair <- sum(z[,l,y] > z[,j,y])
          
          PairSor[j,l,y] <- (bPair + cPair) / (2 * aPair + bPair + cPair)
          PairTurn[j,l,y] <- min(bPair, cPair) / (aPair + min(bPair, cPair))
          PairNest[j,l,y] <- PairSor[j,l,y] - PairTurn[j,l,y]
          
          PairSor[l,j,y] <- PairSor[j,l,y]
          PairTurn[l,j,y] <- PairTurn[j,l,y]
          PairNest[l,j,y] <- PairTurn[j,l,y]
          
        }
      }
    }
    
    for(y in 1:nYears){ 
      for(i in 1:nSpObs){ 
        
        nOcc[i,y] <- sum(z[i,,y]) 
        
      }
    }
  }
})


#####The Non-Annotated Model#####

fireMod <- nimbleCode({
  
  #####Likelihood#####
  
  for(i in 1:nSpObs){
    for(j in 1:nSites){
      for(y in 1:nYears){
        
        if(y == 1){
          
          logit(psi[i,j,y]) <- abar[i] + a[i,j,y] + inprod(bOCV[i,1:nOCV], PatchOCV[j,y,1:nOCV])
          
        } else {
          
          logit(psi[i,j,y]) <- abar[i] + a[i,j,y] + inprod(bOCV[i,1:nOCV], PatchOCV[j,y,1:nOCV]) + theta[i] * z[i,j,(y-1)]
          
        }
        
        z[i,j,y] ~ dbern(psi[i,j,y])
        
        for(s in 1:nTrapWeeks[j,y]){
          
          logit(p[i,j,s,y]) <- lp[i,y] + inprod(bDCV[i,1:nDCV[y],y], SampleDCV[j,s,y,1:nDCV[y]])    
          y[i,j,s,y] ~ dbern(p[i,j,s,y] * z[i,j,y])
          
        }
      }
      
      
      #####Species-Level Priors##### 
      
      #Spatial autocorrelation - Gaussian Process Intercept
      alpha2[i] ~ dexp(1)
      rho2[i] ~ dexp(1)
      delta <- 0.01
      
      for(j in 1:nPatches){
        for(y in 1:nYears){
          z_score[i,j,y] ~ dnorm(0, sd = 1)
        }
      }
      
      for(y in 1:nYears){
        for(j in 1:(nPatches - 1)){
          
          sigma[j,j,i,y] <- alpha2[i] + delta
          
          for(l in (j+1):nPatches){
            
            sigma[j,l,i,y] <- alpha2[i] * exp(-rho2[i] * DMat2[j,l,y])
            sigma[l,j,i,y] <- sigma[j,l,i,y]
            
          }
        }
        
        sigma[nPatches,nPatches,i,y] <- alpha2[i] + delta
        sigma_cd[,,i,y] <- chol(sigma[,,i,y])
        a[i,,y] <- sigma_cd[,,i,y] %*% z_score[i,,y]
        
      }
      
      #Temporal autocorrelation
      theta[i] ~ dnorm(mu.theta, sd = sd.theta)
      
      #Occurence coefficients
      abar[i] ~ dnorm(mu.abar, sd = sd.abar)
      for(n in 1:nOCV){ bOCV[i,n] ~ dnorm(mu.OCV[n], sd = sd.OCV[n]) }
      
      #Detection coefficients
      #Integrated model - separate detection parameters for each sampling method 
      for(y in 1:nYears){
        
        mu.eta[i,y] <- mu.lp[y] + rho[y] * sd.lp[y]/sd.abar * (abar[i] - mu.abar)
        lp[i,y] ~ dnorm(mu.eta[i,y], sd = sd.eta[y])
        
        for(n in 1:nDCV[y]){ 
          
          bDCV[i,n,y] ~ dnorm(mu.DCV[n,y], sd = sd.DCV[n,y]) 
          
        }
      }
    }
    
    #####Community Hyperparameters#####
    
    abar.mean ~ dbeta(1, 1)
    mu.abar <- logit(abar.mean)
    sd.abar ~ dunif(0, 5)
    
    theta.mean ~ dnorm(0, 0.1)
    mu.theta ~ logit(theta.mean)
    sd.theta ~ dunif(0, 5)
    
    for(y in 1:nYears){
      
      p.mean[y] ~ dbeta(1, 1)
      mu.lp[y] <- logit(p.mean[y])
      sd.lp[y] ~ dunif(0, 5)
      
      rho[y] ~ dunif(-1, 1) 
      sd.eta[y] <- sd.lp[y]/rho[y]
      
      for(n in 1:nDCV[y]){
        
        mu.DCV[n,y] ~ dnorm(0, 0.1)
        sd.DCV[n,y] ~ dunif(0, 5)
        
      }
    }
    
    for(n in 1:nOCV){
      
      mu.OCV[n] ~ dnorm(0, 0.1)
      sd.OCV[n] ~ dunif(0, 5)
      
    }
    
    #####Derived Quantities#####
    
    for(j in 1:nPatches){
      
      for(y in 1:nYears){ PatchR[j,y] <- sum(z[,j,y]) }
      
      aEra <- sum(z[,j,1] * z[,j,2])
      bEra <- sum(z[,j,1] > z[,j,2])
      cEra <- sum(z[,j,2] > z[,j,1])
      
      EraSor[j] <- (bEra + cEra) / (2 *  + bEra + cEra)
      EraTurn[j] <- min(bEra, cEra) / (aEra + min(bEra, cEra))
      EraNest[j] <- EraSor[j] - EraTurn[j]
      
    }
    
    for(y in 1:nYears){
      for(j in 1:nPatches){
        
        PairSor[j,j,y] <- 0
        PairTurn[j,j,y] <- 0
        PairNest[j,j,y] <- 0
        
        for(l in (j+1):nPatches){
          
          aPair <- sum(z[,j,y] * z[,l,y])
          bPair <- sum(z[,j,y] > z[,l,y])
          cPair <- sum(z[,l,y] > z[,j,y])
          
          PairSor[j,l,y] <- (bPair + cPair) / (2 * aPair + bPair + cPair)
          PairTurn[j,l,y] <- min(bPair, cPair) / (aPair + min(bPair, cPair))
          PairNest[j,l,y] <- PairSor[j,l,y] - PairTurn[j,l,y]
          
          PairSor[l,j,y] <- PairSor[j,l,y]
          PairTurn[l,j,y] <- PairTurn[j,l,y]
          PairNest[l,j,y] <- PairTurn[j,l,y]
          
        }
      }
    }
    
    for(y in 1:nYears){ 
      for(i in 1:nSpObs){ 
        
        nOcc[i,y] <- sum(z[i,,y]) 
        
      }
    }
  }
})
